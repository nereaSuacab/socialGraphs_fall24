{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the corect description, main actors and more like this section which are the related films\n",
    "(i stopped it at the beggining just to check if the first films info was correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Iterate through each movie URL in the dataset and scrape details\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 93\u001b[0m     scrape_movie_details(url)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Convert the list of dictionaries to a DataFrame and save to CSV\u001b[39;00m\n\u001b[0;32m     96\u001b[0m df_movies \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(movie_data)\n",
      "Cell \u001b[1;32mIn[53], line 24\u001b[0m, in \u001b[0;36mscrape_movie_details\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_movie_details\u001b[39m(url):\n\u001b[0;32m     23\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m---> 24\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Allow time for the page to load\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(driver\u001b[38;5;241m.\u001b[39mpage_source, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Extract details using the same method for JSON-LD data (Director extraction)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Load the list of movies from the CSV file\n",
    "file_path = 'C:/Users/Alba/Desktop/MASTER/4_SEMESTER/Social Graphs/FINAL/movies_info.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# List to store the scraped data\n",
    "movie_data = []\n",
    "\n",
    "# Set up Chrome options to force English language\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--lang=en\")  # Set the browser language to English\n",
    "\n",
    "# Initialize the WebDriver with the options\n",
    "driver = webdriver.Chrome(options=options)  # Make sure ChromeDriver is in your PATH\n",
    "\n",
    "# Function to scrape details from each movie page\n",
    "def scrape_movie_details(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Allow time for the page to load\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Extract details using the same method for JSON-LD data (Director extraction)\n",
    "    try:\n",
    "        # Extract the JSON-LD script\n",
    "        json_ld = driver.execute_script('return document.querySelector(\"script[type=\\'application/ld+json\\']\").innerText')\n",
    "        \n",
    "        # Extract the director from the JSON-LD data\n",
    "        director = json_ld.split('\"director\":')[1].split('}')[0].split('\"name\":\"')[1].split('\"')[0].strip()\n",
    "    except:\n",
    "        director = None\n",
    "    \n",
    "    # Extract other details as needed\n",
    "    try:\n",
    "        title = soup.find('h1').text.strip()\n",
    "    except:\n",
    "        title = None\n",
    "\n",
    "    # Extract 'More Like This' movies (Related films)\n",
    "    try:\n",
    "        more_like_this = [a.text.strip() for a in soup.select('.ipc-poster-card__title')]\n",
    "    except:\n",
    "        more_like_this = None\n",
    "    \n",
    "    try:\n",
    "        # Extract full description including director and actors\n",
    "        description = soup.find('meta', {'name': 'description'})['content']\n",
    "    except:\n",
    "        description = None\n",
    "\n",
    "    # If we have a description, process it\n",
    "    if description:\n",
    "        # Split description into sentences\n",
    "        description_parts = description.split('.')\n",
    "        \n",
    "        # Extract the director from the first sentence\n",
    "        director_info = description_parts[0] if len(description_parts) > 0 else \"\"\n",
    "        director_match = re.search(r'Directed by\\s*([^\\.]+)', director_info)\n",
    "        if director_match:\n",
    "            director = director_match.group(1).strip()\n",
    "\n",
    "        # Extract actors from the second sentence\n",
    "        actors = []\n",
    "        if len(description_parts) > 1:\n",
    "            second_sentence = description_parts[1].strip()  # The second sentence with actors\n",
    "\n",
    "            # Extract actors after \"With\" or \"Starring\"\n",
    "            actor_match = re.search(r'(With|Starring)\\s*([^\\.]+)', second_sentence)\n",
    "            if actor_match:\n",
    "                actors = [actor.strip() for actor in actor_match.group(2).split(',')]\n",
    "        \n",
    "        # Clean the description by removing the director and actors parts\n",
    "        clean_description = '.'.join(description_parts[2:]).strip() if len(description_parts) > 2 else \"\"\n",
    "\n",
    "    # Append data to the movie_data list with try-except to avoid errors disrupting the loop\n",
    "    try:\n",
    "        movie_data.append({\n",
    "            'Title': title,\n",
    "            'URL': url,\n",
    "            'Description': clean_description,\n",
    "            'Actors': actors,\n",
    "            'More Like This': more_like_this,  \n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error appending data for URL: {url} - {e}\")\n",
    "\n",
    "# Iterate through each movie URL in the dataset and scrape details\n",
    "for url in df['URL']:\n",
    "    scrape_movie_details(url)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame and save to CSV\n",
    "df_movies = pd.DataFrame(movie_data)\n",
    "df_movies.to_csv('scraped_movie_data.csv', index=False)\n",
    "\n",
    "# Close the Selenium browser\n",
    "driver.quit()\n",
    "\n",
    "print(\"Data scraping completed and saved to 'scraped_movie_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraping completed and saved to 'scraped_movie_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of dictionaries to a DataFrame and save to CSV\n",
    "df_movies = pd.DataFrame(movie_data)\n",
    "df_movies.to_csv('scraped_movie_data.csv', index=False)\n",
    "\n",
    "# Close the Selenium browser\n",
    "driver.quit()\n",
    "\n",
    "print(\"Data scraping completed and saved to 'scraped_movie_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the description actors and more like this and adding it to the actual csv to have all the info together for the first 10 films just to check!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraping completed and added to the existing CSV for the first 10 films.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib.parse\n",
    "\n",
    "# Load the existing CSV file\n",
    "file_path = 'C:/Users/Alba/Desktop/MASTER/4_SEMESTER/Social Graphs/FINAL/movies_info_copy.csv'\n",
    "df_existing = pd.read_csv(file_path)\n",
    "\n",
    "# List to store the scraped data (will be appended to the existing DataFrame)\n",
    "new_data = []\n",
    "\n",
    "# Set up Chrome options to force English language\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--lang=en\")  # Set the browser language to English\n",
    "\n",
    "# Initialize the WebDriver with the options\n",
    "driver = webdriver.Chrome(options=options)  # Make sure ChromeDriver is in your PATH\n",
    "\n",
    "# Function to scrape details from each movie page\n",
    "def scrape_movie_details(url):\n",
    "    # Ensure the URL is well-formed\n",
    "    if not url or not isinstance(url, str):\n",
    "        print(f\"Skipping invalid URL: {url}\")\n",
    "        return\n",
    "    \n",
    "    # Validate if the URL is a proper URL format (check if it starts with http/https)\n",
    "    if not urllib.parse.urlparse(url).scheme:\n",
    "        print(f\"Skipping malformed URL: {url}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Allow time for the page to load\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Extract 'More Like This' movies\n",
    "        try:\n",
    "            more_like_this = [a.text.strip() for a in soup.select('.ipc-poster-card__title')]\n",
    "        except:\n",
    "            more_like_this = None\n",
    "        \n",
    "        try:\n",
    "            # Extract full description including director and actors\n",
    "            description = soup.find('meta', {'name': 'description'})['content']\n",
    "        except:\n",
    "            description = None\n",
    "\n",
    "        # If we have a description, process it\n",
    "        if description:\n",
    "            # Split description into sentences\n",
    "            description_parts = description.split('.')\n",
    "            \n",
    "            # Extract the director from the first sentence\n",
    "            director_info = description_parts[0] if len(description_parts) > 0 else \"\"\n",
    "            director_match = re.search(r'Directed by\\s*([^\\.]+)', director_info)\n",
    "            if director_match:\n",
    "                director = director_match.group(1).strip()\n",
    "\n",
    "            # Extract actors from the second sentence\n",
    "            actors = []\n",
    "            if len(description_parts) > 1:\n",
    "                second_sentence = description_parts[1].strip()  # The second sentence with actors\n",
    "\n",
    "                # Extract actors after \"With\" or \"Starring\"\n",
    "                actor_match = re.search(r'(With|Starring)\\s*([^\\.]+)', second_sentence)\n",
    "                if actor_match:\n",
    "                    actors = [actor.strip() for actor in actor_match.group(2).split(',')]\n",
    "            \n",
    "            # Clean the description by removing the director and actors parts\n",
    "            clean_description = '.'.join(description_parts[2:]).strip() if len(description_parts) > 2 else \"\"\n",
    "\n",
    "\n",
    "        # Append data to the new_data list (in the format to match the existing CSV)\n",
    "        try:\n",
    "            new_data.append({\n",
    "                'URL': url,\n",
    "                'Description_good': clean_description,\n",
    "                'Actors': actors,\n",
    "                'More Like This': more_like_this,  \n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error appending data for URL: {url} - {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "\n",
    "# Limit the iteration to the first 10 films\n",
    "urls_to_scrape = df_existing['URL'][:10]\n",
    "\n",
    "# Iterate through each movie URL in the first 10 rows and scrape details\n",
    "for url in urls_to_scrape:\n",
    "    scrape_movie_details(url)\n",
    "\n",
    "# Convert the new data list to a DataFrame\n",
    "df_new_data = pd.DataFrame(new_data)\n",
    "\n",
    "# Merge the new data with the existing data using the 'URL' column as the key\n",
    "df_updated = pd.merge(df_existing, df_new_data, on='URL', how='left')\n",
    "\n",
    "# Save the updated DataFrame back to the same CSV file (you can change the filename if needed)\n",
    "df_updated.to_csv(file_path, index=False)\n",
    "\n",
    "# Close the Selenium browser\n",
    "driver.quit()\n",
    "\n",
    "print(\"Data scraping completed and added to the existing CSV for the first 10 films.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the description actors and more like this and adding it to the actual csv to have all the info together for all the 787 films!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib.parse\n",
    "\n",
    "# Load the existing CSV file\n",
    "file_path = 'C:/Users/Alba/Desktop/MASTER/4_SEMESTER/Social Graphs/FINAL/movies_info_copy.csv'\n",
    "df_existing = pd.read_csv(file_path)\n",
    "\n",
    "# List to store the scraped data (will be appended to the existing DataFrame)\n",
    "new_data = []\n",
    "\n",
    "# Set up Chrome options to force English language\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--lang=en\")  # Set the browser language to English\n",
    "\n",
    "# Initialize the WebDriver with the options\n",
    "driver = webdriver.Chrome(options=options)  # Make sure ChromeDriver is in your PATH\n",
    "\n",
    "# Function to scrape details from each movie page\n",
    "def scrape_movie_details(url):\n",
    "    # Ensure the URL is well-formed\n",
    "    if not url or not isinstance(url, str):\n",
    "        print(f\"Skipping invalid URL: {url}\")\n",
    "        return\n",
    "    \n",
    "    # Validate if the URL is a proper URL format (check if it starts with http/https)\n",
    "    if not urllib.parse.urlparse(url).scheme:\n",
    "        print(f\"Skipping malformed URL: {url}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Allow time for the page to load\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Extract details using the same method for JSON-LD data (Director extraction)\n",
    "        try:\n",
    "            # Extract the JSON-LD script\n",
    "            json_ld = driver.execute_script('return document.querySelector(\"script[type=\\'application/ld+json\\']\").innerText')\n",
    "            \n",
    "            # Extract the director from the JSON-LD data\n",
    "            director = json_ld.split('\"director\":')[1].split('}')[0].split('\"name\":\"')[1].split('\"')[0].strip()\n",
    "        except:\n",
    "            director = None\n",
    "        \n",
    "        # Extract other details as needed\n",
    "        try:\n",
    "            title = soup.find('h1').text.strip()\n",
    "        except:\n",
    "            title = None\n",
    "\n",
    "        # Extract 'More Like This' movies (Related films)\n",
    "        try:\n",
    "            more_like_this = [a.text.strip() for a in soup.select('.ipc-poster-card__title')]\n",
    "        except:\n",
    "            more_like_this = None\n",
    "        \n",
    "        try:\n",
    "            # Extract full description including director and actors\n",
    "            description = soup.find('meta', {'name': 'description'})['content']\n",
    "        except:\n",
    "            description = None\n",
    "\n",
    "        # If we have a description, process it\n",
    "        if description:\n",
    "            # Split description into sentences\n",
    "            description_parts = description.split('.')\n",
    "            \n",
    "            if len(description_parts) > 1:\n",
    "                second_sentence = description_parts[1].strip()  # Get the second sentence (actors info)\n",
    "\n",
    "                # Extract actors from the second sentence (assuming the format includes 'Starring' or 'Starring: ' before actors' names)\n",
    "                actors = re.findall(r'(?:Starring|Con):?\\s*([^\\.]+)', second_sentence)\n",
    "\n",
    "                # Clean up the list of actors (if needed)\n",
    "                actors = [actor.strip() for actor in actors[0].split(',')] if actors else []\n",
    "\n",
    "                # Remove the second sentence (actors' names) from the description\n",
    "                clean_description = '.'.join(description_parts[2:]).strip() if len(description_parts) > 2 else \"\"\n",
    "            \n",
    "            else:\n",
    "                clean_description = description.strip()\n",
    "                actors = []\n",
    "\n",
    "        # Append data to the new_data list (in the format to match the existing CSV)\n",
    "        try:\n",
    "            new_data.append({\n",
    "                'URL': url,\n",
    "                'Description_good': clean_description,\n",
    "                'Actors': actors,\n",
    "                'More Like This': more_like_this,  \n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error appending data for URL: {url} - {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "\n",
    "# Iterate through each movie URL in the existing dataset and scrape details\n",
    "for url in df_existing['URL']:\n",
    "    scrape_movie_details(url)\n",
    "\n",
    "# Convert the new data list to a DataFrame\n",
    "df_new_data = pd.DataFrame(new_data)\n",
    "\n",
    "# Merge the new data with the existing data using the 'URL' column as the key\n",
    "df_updated = pd.merge(df_existing, df_new_data, on='URL', how='left')\n",
    "\n",
    "# Save the updated DataFrame back to the same CSV file (you can change the filename if needed)\n",
    "df_updated.to_csv(file_path, index=False)\n",
    "\n",
    "# Close the Selenium browser\n",
    "driver.quit()\n",
    "\n",
    "print(\"Data scraping completed and added to the existing CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialgraphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
